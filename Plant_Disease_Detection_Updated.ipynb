{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN NEURAL NETWORK WITH ADAM OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Number of training samples: 1322\n",
      "Number of validation samples: 60\n",
      "Number of test samples: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def preprocess_images(data_dir, target_size=(28, 28), color_mode='grayscale'):\n",
    "\n",
    "    X, y = [], []\n",
    "    class_labels = os.listdir(data_dir)  # Folder names are the labels\n",
    "    print(f\"Found class labels: {class_labels}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(class_labels)\n",
    "    print(f\"Encoded labels: {encoded_labels}\")\n",
    "\n",
    "    for label, encoded_label in zip(class_labels, encoded_labels):\n",
    "        folder_path = os.path.join(data_dir, label)\n",
    "        if not os.path.exists(folder_path) or not os.listdir(folder_path):\n",
    "            print(f\"Warning: Folder {folder_path} is empty or does not exist.\")\n",
    "            continue\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    img = Image.open(file_path)\n",
    "                    if color_mode == 'grayscale':\n",
    "                        img = img.convert('L')  # Convert to grayscale\n",
    "                    img = img.resize(target_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(encoded_label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    if not X or not y:\n",
    "        raise ValueError(f\"No valid data found in directory: {data_dir}\")\n",
    "    \n",
    "    X = np.array(X).astype('float32') / 255.0  # Normalize pixel values\n",
    "    y = to_categorical(y)  # One-hot encode labels\n",
    "    return X, y, label_encoder\n",
    "\n",
    "# Paths to your dataset directories\n",
    "train_path = 'C:\\Programming\\Machine learning\\plant images\\Train\\Train'\n",
    "valid_path = 'C:\\Programming\\Machine learning\\plant images\\Validation\\Validation'\n",
    "test_path = 'C:\\Programming\\Machine learning\\plant images\\Test\\Test'\n",
    "\n",
    "# Preprocess datasets\n",
    "X_train, y_train, label_encoder = preprocess_images(train_path, target_size=(28, 28))\n",
    "X_valid, y_valid, _ = preprocess_images(valid_path, target_size=(28, 28))\n",
    "X_test, y_test, _ = preprocess_images(test_path, target_size=(28, 28))\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of validation samples: {X_valid.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.3562 - loss: 1.1020 - precision: 0.0000e+00 - recall: 0.0000e+00 - val_accuracy: 0.4167 - val_loss: 1.0709 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00\n",
      "Epoch 2/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.4777 - loss: 1.0539 - precision: 0.3678 - recall: 0.0381 - val_accuracy: 0.5000 - val_loss: 0.9636 - val_precision: 0.8000 - val_recall: 0.1333\n",
      "Epoch 3/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.5264 - loss: 0.9798 - precision: 0.6402 - recall: 0.2305 - val_accuracy: 0.6500 - val_loss: 0.8376 - val_precision: 0.7500 - val_recall: 0.6000\n",
      "Epoch 4/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5716 - loss: 0.9054 - precision: 0.6552 - recall: 0.4320 - val_accuracy: 0.6000 - val_loss: 0.7854 - val_precision: 0.7179 - val_recall: 0.4667\n",
      "Epoch 5/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.6747 - loss: 0.7600 - precision: 0.7278 - recall: 0.5795 - val_accuracy: 0.5667 - val_loss: 0.8194 - val_precision: 0.6667 - val_recall: 0.4333\n",
      "Epoch 6/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6549 - loss: 0.7647 - precision: 0.7304 - recall: 0.5244 - val_accuracy: 0.5333 - val_loss: 0.8093 - val_precision: 0.6250 - val_recall: 0.5000\n",
      "Epoch 7/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6981 - loss: 0.7107 - precision: 0.7599 - recall: 0.6057 - val_accuracy: 0.6167 - val_loss: 0.7866 - val_precision: 0.6296 - val_recall: 0.5667\n",
      "Epoch 8/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.7532 - loss: 0.6158 - precision: 0.7812 - recall: 0.7107 - val_accuracy: 0.7333 - val_loss: 0.6708 - val_precision: 0.7600 - val_recall: 0.6333\n",
      "Epoch 9/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7321 - loss: 0.6316 - precision: 0.7712 - recall: 0.6854 - val_accuracy: 0.7500 - val_loss: 0.6745 - val_precision: 0.7843 - val_recall: 0.6667\n",
      "Epoch 10/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7582 - loss: 0.5846 - precision: 0.7824 - recall: 0.7154 - val_accuracy: 0.7500 - val_loss: 0.6416 - val_precision: 0.7925 - val_recall: 0.7000\n",
      "Epoch 11/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.7800 - loss: 0.5050 - precision: 0.8124 - recall: 0.7603 - val_accuracy: 0.7000 - val_loss: 0.5995 - val_precision: 0.8000 - val_recall: 0.6667\n",
      "Epoch 12/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8103 - loss: 0.5217 - precision: 0.8291 - recall: 0.7685 - val_accuracy: 0.7500 - val_loss: 0.6736 - val_precision: 0.7544 - val_recall: 0.7167\n",
      "Epoch 13/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.7962 - loss: 0.5173 - precision: 0.8087 - recall: 0.7552 - val_accuracy: 0.7167 - val_loss: 0.6409 - val_precision: 0.7593 - val_recall: 0.6833\n",
      "Epoch 14/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8172 - loss: 0.4826 - precision: 0.8294 - recall: 0.7857 - val_accuracy: 0.7833 - val_loss: 0.5692 - val_precision: 0.8148 - val_recall: 0.7333\n",
      "Epoch 15/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8377 - loss: 0.4380 - precision: 0.8460 - recall: 0.8177 - val_accuracy: 0.7667 - val_loss: 0.5746 - val_precision: 0.7679 - val_recall: 0.7167\n",
      "Epoch 16/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8424 - loss: 0.4277 - precision: 0.8591 - recall: 0.8011 - val_accuracy: 0.7000 - val_loss: 0.6811 - val_precision: 0.7778 - val_recall: 0.7000\n",
      "Epoch 17/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8218 - loss: 0.4697 - precision: 0.8357 - recall: 0.7906 - val_accuracy: 0.7667 - val_loss: 0.5511 - val_precision: 0.7857 - val_recall: 0.7333\n",
      "Epoch 18/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8135 - loss: 0.4502 - precision: 0.8288 - recall: 0.7869 - val_accuracy: 0.7333 - val_loss: 0.5826 - val_precision: 0.8148 - val_recall: 0.7333\n",
      "Epoch 19/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8467 - loss: 0.3922 - precision: 0.8559 - recall: 0.8280 - val_accuracy: 0.7167 - val_loss: 0.5876 - val_precision: 0.7736 - val_recall: 0.6833\n",
      "Epoch 20/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8239 - loss: 0.4275 - precision: 0.8298 - recall: 0.8083 - val_accuracy: 0.8167 - val_loss: 0.5637 - val_precision: 0.8070 - val_recall: 0.7667\n",
      "Epoch 21/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8611 - loss: 0.3552 - precision: 0.8814 - recall: 0.8432 - val_accuracy: 0.7333 - val_loss: 0.5756 - val_precision: 0.7778 - val_recall: 0.7000\n",
      "Epoch 22/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8611 - loss: 0.3410 - precision: 0.8816 - recall: 0.8432 - val_accuracy: 0.7333 - val_loss: 0.5353 - val_precision: 0.7719 - val_recall: 0.7333\n",
      "Epoch 23/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8612 - loss: 0.3285 - precision: 0.8804 - recall: 0.8494 - val_accuracy: 0.7000 - val_loss: 0.7991 - val_precision: 0.6949 - val_recall: 0.6833\n",
      "Epoch 24/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8396 - loss: 0.3555 - precision: 0.8519 - recall: 0.8219 - val_accuracy: 0.8000 - val_loss: 0.5668 - val_precision: 0.7966 - val_recall: 0.7833\n",
      "Epoch 25/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8738 - loss: 0.2918 - precision: 0.8851 - recall: 0.8634 - val_accuracy: 0.7333 - val_loss: 0.6374 - val_precision: 0.7458 - val_recall: 0.7333\n",
      "Epoch 26/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8637 - loss: 0.3381 - precision: 0.8777 - recall: 0.8514 - val_accuracy: 0.7333 - val_loss: 0.5632 - val_precision: 0.7544 - val_recall: 0.7167\n",
      "Epoch 27/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8754 - loss: 0.3194 - precision: 0.8866 - recall: 0.8643 - val_accuracy: 0.7333 - val_loss: 0.6934 - val_precision: 0.7288 - val_recall: 0.7167\n",
      "Epoch 28/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9153 - loss: 0.2472 - precision: 0.9218 - recall: 0.9053 - val_accuracy: 0.7333 - val_loss: 0.7050 - val_precision: 0.7288 - val_recall: 0.7167\n",
      "Epoch 29/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9265 - loss: 0.2217 - precision: 0.9338 - recall: 0.9204 - val_accuracy: 0.7500 - val_loss: 0.5643 - val_precision: 0.7586 - val_recall: 0.7333\n",
      "Epoch 30/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8891 - loss: 0.2641 - precision: 0.9040 - recall: 0.8767 - val_accuracy: 0.7667 - val_loss: 0.6173 - val_precision: 0.7857 - val_recall: 0.7333\n",
      "Epoch 31/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.8920 - loss: 0.2540 - precision: 0.9061 - recall: 0.8857 - val_accuracy: 0.7167 - val_loss: 0.6175 - val_precision: 0.7288 - val_recall: 0.7167\n",
      "Epoch 32/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9048 - loss: 0.2430 - precision: 0.9064 - recall: 0.8955 - val_accuracy: 0.7333 - val_loss: 0.6129 - val_precision: 0.7857 - val_recall: 0.7333\n",
      "Epoch 33/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9172 - loss: 0.2251 - precision: 0.9226 - recall: 0.9069 - val_accuracy: 0.6667 - val_loss: 0.9573 - val_precision: 0.6780 - val_recall: 0.6667\n",
      "Epoch 34/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.8951 - loss: 0.2498 - precision: 0.8976 - recall: 0.8818 - val_accuracy: 0.6500 - val_loss: 0.7710 - val_precision: 0.6610 - val_recall: 0.6500\n",
      "Epoch 35/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9347 - loss: 0.1861 - precision: 0.9392 - recall: 0.9263 - val_accuracy: 0.7167 - val_loss: 0.6724 - val_precision: 0.7119 - val_recall: 0.7000\n",
      "Epoch 36/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9579 - loss: 0.1384 - precision: 0.9581 - recall: 0.9538 - val_accuracy: 0.7167 - val_loss: 0.8704 - val_precision: 0.7119 - val_recall: 0.7000\n",
      "Epoch 37/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9522 - loss: 0.1543 - precision: 0.9522 - recall: 0.9459 - val_accuracy: 0.7667 - val_loss: 0.7938 - val_precision: 0.7586 - val_recall: 0.7333\n",
      "Epoch 38/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9601 - loss: 0.1338 - precision: 0.9628 - recall: 0.9575 - val_accuracy: 0.6667 - val_loss: 0.9638 - val_precision: 0.6897 - val_recall: 0.6667\n",
      "Epoch 39/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9502 - loss: 0.1513 - precision: 0.9531 - recall: 0.9464 - val_accuracy: 0.6833 - val_loss: 0.8395 - val_precision: 0.6724 - val_recall: 0.6500\n",
      "Epoch 40/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9646 - loss: 0.1133 - precision: 0.9682 - recall: 0.9609 - val_accuracy: 0.7167 - val_loss: 0.8449 - val_precision: 0.7288 - val_recall: 0.7167\n",
      "Epoch 41/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9598 - loss: 0.1083 - precision: 0.9608 - recall: 0.9581 - val_accuracy: 0.7167 - val_loss: 0.9224 - val_precision: 0.7167 - val_recall: 0.7167\n",
      "Epoch 42/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9781 - loss: 0.0828 - precision: 0.9826 - recall: 0.9769 - val_accuracy: 0.7167 - val_loss: 0.7961 - val_precision: 0.7167 - val_recall: 0.7167\n",
      "Epoch 43/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9858 - loss: 0.0722 - precision: 0.9873 - recall: 0.9825 - val_accuracy: 0.6667 - val_loss: 1.0375 - val_precision: 0.6667 - val_recall: 0.6667\n",
      "Epoch 44/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9689 - loss: 0.0874 - precision: 0.9716 - recall: 0.9649 - val_accuracy: 0.7333 - val_loss: 0.9023 - val_precision: 0.7333 - val_recall: 0.7333\n",
      "Epoch 45/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9820 - loss: 0.0694 - precision: 0.9820 - recall: 0.9820 - val_accuracy: 0.7333 - val_loss: 1.0127 - val_precision: 0.7288 - val_recall: 0.7167\n",
      "Epoch 46/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9804 - loss: 0.0629 - precision: 0.9809 - recall: 0.9783 - val_accuracy: 0.6833 - val_loss: 1.0816 - val_precision: 0.6833 - val_recall: 0.6833\n",
      "Epoch 47/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9913 - loss: 0.0479 - precision: 0.9913 - recall: 0.9902 - val_accuracy: 0.6667 - val_loss: 1.1331 - val_precision: 0.6667 - val_recall: 0.6667\n",
      "Epoch 48/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9900 - loss: 0.0521 - precision: 0.9899 - recall: 0.9894 - val_accuracy: 0.7000 - val_loss: 1.1795 - val_precision: 0.7241 - val_recall: 0.7000\n",
      "Epoch 49/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9754 - loss: 0.0723 - precision: 0.9753 - recall: 0.9740 - val_accuracy: 0.6833 - val_loss: 1.2382 - val_precision: 0.6897 - val_recall: 0.6667\n",
      "Epoch 50/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9811 - loss: 0.0546 - precision: 0.9820 - recall: 0.9811 - val_accuracy: 0.7333 - val_loss: 1.1680 - val_precision: 0.7288 - val_recall: 0.7167\n"
     ]
    }
   ],
   "source": [
    "# Define your model here (example using a simple CNN)\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(28, 28, 1)),  # Adjust input shape for grayscale images\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3 classes: Healthy, Powdery, Rust\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6169 - loss: 1.6206 - precision: 0.6240 - recall: 0.6134 \n",
      "Test Loss: 1.8584\n",
      "Test Accuracy: 0.5733\n",
      "Test Precision: 0.5743\n",
      "Test Recall: 0.5667\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test Recall: {test_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM NEURAL NETWORK WITH ADAM OPTIMIZER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Number of training samples: 1322\n",
      "Number of validation samples: 60\n",
      "Number of test samples: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def preprocess_images_for_lstm(data_dir, target_size=(28, 28), color_mode='grayscale'):\n",
    "\n",
    "    X, y = [], []\n",
    "    class_labels = os.listdir(data_dir)  # Folder names are the labels\n",
    "    print(f\"Found class labels: {class_labels}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(class_labels)\n",
    "    print(f\"Encoded labels: {encoded_labels}\")\n",
    "\n",
    "    for label, encoded_label in zip(class_labels, encoded_labels):\n",
    "        folder_path = os.path.join(data_dir, label)\n",
    "        if not os.path.exists(folder_path) or not os.listdir(folder_path):\n",
    "            print(f\"Warning: Folder {folder_path} is empty or does not exist.\")\n",
    "            continue\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    img = Image.open(file_path)\n",
    "                    if color_mode == 'grayscale':\n",
    "                        img = img.convert('L')  # Convert to grayscale\n",
    "                    img = img.resize(target_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(encoded_label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    if not X or not y:\n",
    "        raise ValueError(f\"No valid data found in directory: {data_dir}\")\n",
    "    \n",
    "    X = np.array(X).astype('float32') / 255.0  # Normalize pixel values\n",
    "    y = to_categorical(y)  # One-hot encode labels\n",
    "\n",
    "    # Reshape images into sequences for LSTM input\n",
    "    # Each row of the image becomes a \"time step\" in the sequence\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2])\n",
    "    return X, y, label_encoder\n",
    "\n",
    "# Paths to your dataset directories\n",
    "train_path = 'C:\\Programming\\Machine learning\\plant images\\Train\\Train'\n",
    "valid_path = 'C:\\Programming\\Machine learning\\plant images\\Validation\\Validation'\n",
    "test_path = 'C:\\Programming\\Machine learning\\plant images\\Test\\Test'\n",
    "\n",
    "# Preprocess datasets\n",
    "X_train, y_train, label_encoder = preprocess_images_for_lstm(train_path, target_size=(28, 28))\n",
    "X_valid, y_valid, _ = preprocess_images_for_lstm(valid_path, target_size=(28, 28))\n",
    "X_test, y_test, _ = preprocess_images_for_lstm(test_path, target_size=(28, 28))\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of validation samples: {X_valid.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.3437 - loss: 1.1000 - val_accuracy: 0.3000 - val_loss: 1.1027\n",
      "Epoch 2/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.4913 - loss: 1.0130 - val_accuracy: 0.3333 - val_loss: 1.1397\n",
      "Epoch 3/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5304 - loss: 0.9863 - val_accuracy: 0.4667 - val_loss: 1.0871\n",
      "Epoch 4/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5416 - loss: 0.9881 - val_accuracy: 0.3500 - val_loss: 1.1629\n",
      "Epoch 5/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.4814 - loss: 1.0003 - val_accuracy: 0.3667 - val_loss: 1.1531\n",
      "Epoch 6/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5807 - loss: 0.9245 - val_accuracy: 0.3500 - val_loss: 1.1225\n",
      "Epoch 7/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6139 - loss: 0.8411 - val_accuracy: 0.3833 - val_loss: 1.0437\n",
      "Epoch 8/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6194 - loss: 0.8396 - val_accuracy: 0.4167 - val_loss: 1.0619\n",
      "Epoch 9/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6687 - loss: 0.7961 - val_accuracy: 0.4833 - val_loss: 1.0174\n",
      "Epoch 10/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6558 - loss: 0.8043 - val_accuracy: 0.5333 - val_loss: 0.9675\n",
      "Epoch 11/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6391 - loss: 0.8955 - val_accuracy: 0.3500 - val_loss: 1.0635\n",
      "Epoch 12/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5676 - loss: 0.9006 - val_accuracy: 0.4500 - val_loss: 0.9543\n",
      "Epoch 13/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6462 - loss: 0.7961 - val_accuracy: 0.5000 - val_loss: 1.0243\n",
      "Epoch 14/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6771 - loss: 0.7277 - val_accuracy: 0.5167 - val_loss: 0.9167\n",
      "Epoch 15/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6653 - loss: 0.7749 - val_accuracy: 0.4500 - val_loss: 0.9789\n",
      "Epoch 16/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6128 - loss: 0.8617 - val_accuracy: 0.5667 - val_loss: 0.9399\n",
      "Epoch 17/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6584 - loss: 0.7710 - val_accuracy: 0.5167 - val_loss: 0.8669\n",
      "Epoch 18/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6779 - loss: 0.7320 - val_accuracy: 0.4833 - val_loss: 0.9747\n",
      "Epoch 19/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6613 - loss: 0.7628 - val_accuracy: 0.5167 - val_loss: 0.9026\n",
      "Epoch 20/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6714 - loss: 0.7293 - val_accuracy: 0.6333 - val_loss: 0.8192\n",
      "Epoch 21/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6666 - loss: 0.7770 - val_accuracy: 0.5667 - val_loss: 0.8781\n",
      "Epoch 22/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6933 - loss: 0.6946 - val_accuracy: 0.4833 - val_loss: 0.9156\n",
      "Epoch 23/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7090 - loss: 0.6876 - val_accuracy: 0.6000 - val_loss: 0.8458\n",
      "Epoch 24/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.7192 - loss: 0.6740 - val_accuracy: 0.6167 - val_loss: 0.8243\n",
      "Epoch 25/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7134 - loss: 0.6484 - val_accuracy: 0.5667 - val_loss: 0.8823\n",
      "Epoch 26/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6634 - loss: 3.6883 - val_accuracy: 0.5167 - val_loss: 0.9905\n",
      "Epoch 27/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5038 - loss: 1.0206 - val_accuracy: 0.5333 - val_loss: 1.0338\n",
      "Epoch 28/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.5512 - loss: 0.9348 - val_accuracy: 0.5833 - val_loss: 0.8442\n",
      "Epoch 29/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.5721 - loss: 0.8935 - val_accuracy: 0.5667 - val_loss: 0.9203\n",
      "Epoch 30/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6075 - loss: 0.8707 - val_accuracy: 0.6167 - val_loss: 0.7834\n",
      "Epoch 31/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6074 - loss: 0.8557 - val_accuracy: 0.5833 - val_loss: 0.8597\n",
      "Epoch 32/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6544 - loss: 0.7978 - val_accuracy: 0.6667 - val_loss: 0.8410\n",
      "Epoch 33/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6612 - loss: 0.7729 - val_accuracy: 0.5333 - val_loss: 1.0784\n",
      "Epoch 34/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6382 - loss: 0.7808 - val_accuracy: 0.5500 - val_loss: 0.8435\n",
      "Epoch 35/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.7024 - loss: 0.7120 - val_accuracy: 0.6167 - val_loss: 0.7614\n",
      "Epoch 36/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6723 - loss: 0.7278 - val_accuracy: 0.5167 - val_loss: 0.9502\n",
      "Epoch 37/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.6706 - loss: 0.7312 - val_accuracy: 0.6167 - val_loss: 0.7514\n",
      "Epoch 38/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6679 - loss: 0.7305 - val_accuracy: 0.6667 - val_loss: 0.7104\n",
      "Epoch 39/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6986 - loss: 0.6804 - val_accuracy: 0.5833 - val_loss: 0.8350\n",
      "Epoch 40/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7013 - loss: 0.6925 - val_accuracy: 0.5333 - val_loss: 0.8507\n",
      "Epoch 41/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6911 - loss: 0.6860 - val_accuracy: 0.5000 - val_loss: 0.9046\n",
      "Epoch 42/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7140 - loss: 0.6823 - val_accuracy: 0.6000 - val_loss: 0.7745\n",
      "Epoch 43/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7330 - loss: 0.6658 - val_accuracy: 0.5500 - val_loss: 0.8561\n",
      "Epoch 44/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7550 - loss: 0.6216 - val_accuracy: 0.5833 - val_loss: 0.7543\n",
      "Epoch 45/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.7372 - loss: 0.6115 - val_accuracy: 0.5167 - val_loss: 0.8809\n",
      "Epoch 46/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7350 - loss: 0.6225 - val_accuracy: 0.6167 - val_loss: 0.7635\n",
      "Epoch 47/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6641 - loss: 0.8311 - val_accuracy: 0.5167 - val_loss: 0.8745\n",
      "Epoch 48/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6971 - loss: 0.7672 - val_accuracy: 0.5333 - val_loss: 0.8663\n",
      "Epoch 49/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7236 - loss: 0.6718 - val_accuracy: 0.4333 - val_loss: 1.0595\n",
      "Epoch 50/50\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.7141 - loss: 0.6715 - val_accuracy: 0.4667 - val_loss: 1.2150\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models, layers, optimizers\n",
    "\n",
    "# Define LSTM model\n",
    "model = models.Sequential([\n",
    "    layers.InputLayer(input_shape=(28, 28)),  # Input shape: (time steps, features)\n",
    "    layers.LSTM(128, return_sequences=True, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.LSTM(64, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # 3 classes: Healthy, Powdery, Rust\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,  # Reduce or increase based on convergence\n",
    "    batch_size=32,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4790 - loss: 1.2776 \n",
      "Test accuracy: 0.4200\n",
      "Test loss: 1.3475\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN + LSTM NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Number of training samples: 1322\n",
      "Number of validation samples: 60\n",
      "Number of test samples: 150\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Modify the preprocessing step to ensure the correct shape (height, width, channels)\n",
    "def preprocess_images(data_dir, target_size=(28, 28), color_mode='grayscale'):\n",
    "    \"\"\"\n",
    "    Preprocesses the images in the given directory.\n",
    "    - Resizes images to `target_size`.\n",
    "    - Converts to grayscale if `color_mode` is 'grayscale'.\n",
    "    - Normalizes pixel values to [0, 1].\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    class_labels = os.listdir(data_dir)  # Folder names are the labels\n",
    "    print(f\"Found class labels: {class_labels}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(class_labels)\n",
    "    print(f\"Encoded labels: {encoded_labels}\")\n",
    "\n",
    "    for label, encoded_label in zip(class_labels, encoded_labels):\n",
    "        folder_path = os.path.join(data_dir, label)\n",
    "        if not os.path.exists(folder_path) or not os.listdir(folder_path):\n",
    "            print(f\"Warning: Folder {folder_path} is empty or does not exist.\")\n",
    "            continue\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    img = Image.open(file_path)\n",
    "                    if color_mode == 'grayscale':\n",
    "                        img = img.convert('L')  # Convert to grayscale\n",
    "                    img = img.resize(target_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(encoded_label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    if not X or not y:\n",
    "        raise ValueError(f\"No valid data found in directory: {data_dir}\")\n",
    "    \n",
    "    X = np.array(X).astype('float32') / 255.0  # Normalize pixel values\n",
    "    X = np.expand_dims(X, axis=-1)  # Add the channel dimension (grayscale -> (28, 28, 1))\n",
    "    y = to_categorical(y)  # One-hot encode labels\n",
    "    return X, y, label_encoder\n",
    "\n",
    "# Paths to your dataset directories\n",
    "train_path = 'C:/Programming/Machine learning/plant images/Train/Train'\n",
    "valid_path = 'C:/Programming/Machine learning/plant images/Validation/Validation'\n",
    "test_path = 'C:/Programming/Machine learning/plant images/Test/Test'\n",
    "\n",
    "# Preprocess datasets\n",
    "X_train, y_train, label_encoder = preprocess_images(train_path, target_size=(28, 28))\n",
    "X_valid, y_valid, _ = preprocess_images(valid_path, target_size=(28, 28))\n",
    "X_test, y_test, _ = preprocess_images(test_path, target_size=(28, 28))\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of validation samples: {X_valid.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")\n",
    "\n",
    "# Data Augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  # Randomly rotate images\n",
    "    width_shift_range=0.2,  # Randomly shift images horizontally\n",
    "    height_shift_range=0.2,  # Randomly shift images vertically\n",
    "    zoom_range=0.2,  # Randomly zoom in images\n",
    "    horizontal_flip=True,  # Randomly flip images horizontally\n",
    "    fill_mode='nearest'  # Fill pixels after transformations\n",
    ")\n",
    "\n",
    "# Fit the data generator on the training data\n",
    "datagen.fit(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1322 images belonging to 3 classes.\n",
      "Found 60 images belonging to 3 classes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4608</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">36</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_13 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_20 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_14 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_21 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_15 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_2 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4608\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ reshape_4 (\u001b[38;5;33mReshape\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m36\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m49,408\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m4,160\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,915</span> (1.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m278,915\u001b[0m (1.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">278,467</span> (1.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m278,467\u001b[0m (1.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Assuming your data is prepared with proper train, val, and test sets\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=30, width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2, shear_range=0.2, zoom_range=0.2, \n",
    "                                   horizontal_flip=True, fill_mode='nearest')\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_path,\n",
    "                                                    target_size=(64, 64),  # Adjust size to avoid negative dimension\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical', \n",
    "                                                    color_mode='grayscale')  # Adjust for grayscale if needed\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_generator = val_datagen.flow_from_directory(valid_path,\n",
    "                                                target_size=(64, 64),\n",
    "                                                batch_size=32,\n",
    "                                                class_mode='categorical', \n",
    "                                                color_mode='grayscale')\n",
    "\n",
    "# Define the CNN + LSTM model\n",
    "model = models.Sequential([\n",
    "    # CNN layers for feature extraction\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 1)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the CNN output and reshape for LSTM\n",
    "    layers.Flatten(),\n",
    "    layers.Reshape((-1, 128)),  # Flatten to sequences suitable for LSTM\n",
    "\n",
    "    # LSTM layers\n",
    "    layers.LSTM(128, return_sequences=True),\n",
    "    layers.LSTM(64),\n",
    "    \n",
    "    # Fully connected layers\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')  # Adjust the number of classes to 3 (Healthy, Powdery, Rust)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'Precision', 'Recall'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - Precision: 0.8401 - Recall: 0.8012 - accuracy: 0.8207 - loss: 0.4440 - val_Precision: 0.7091 - val_Recall: 0.6500 - val_accuracy: 0.7000 - val_loss: 0.9923\n",
      "Epoch 2/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - Precision: 0.8479 - Recall: 0.8079 - accuracy: 0.8229 - loss: 0.4337 - val_Precision: 0.5690 - val_Recall: 0.5500 - val_accuracy: 0.5833 - val_loss: 1.1593\n",
      "Epoch 3/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 2s/step - Precision: 0.8661 - Recall: 0.8113 - accuracy: 0.8408 - loss: 0.4207 - val_Precision: 0.8136 - val_Recall: 0.8000 - val_accuracy: 0.8167 - val_loss: 0.5017\n",
      "Epoch 4/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 2s/step - Precision: 0.8510 - Recall: 0.7962 - accuracy: 0.8231 - loss: 0.4392 - val_Precision: 0.6429 - val_Recall: 0.6000 - val_accuracy: 0.6000 - val_loss: 0.9745\n",
      "Epoch 5/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 2s/step - Precision: 0.8734 - Recall: 0.8498 - accuracy: 0.8629 - loss: 0.3812 - val_Precision: 0.5333 - val_Recall: 0.5333 - val_accuracy: 0.5333 - val_loss: 1.5341\n",
      "Epoch 6/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 2s/step - Precision: 0.8623 - Recall: 0.8422 - accuracy: 0.8524 - loss: 0.3881 - val_Precision: 0.4259 - val_Recall: 0.3833 - val_accuracy: 0.3833 - val_loss: 1.2804\n",
      "Epoch 7/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 2s/step - Precision: 0.8728 - Recall: 0.8380 - accuracy: 0.8534 - loss: 0.3749 - val_Precision: 0.5000 - val_Recall: 0.5000 - val_accuracy: 0.5000 - val_loss: 2.4747\n",
      "Epoch 8/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - Precision: 0.8719 - Recall: 0.8288 - accuracy: 0.8475 - loss: 0.3934 - val_Precision: 0.5254 - val_Recall: 0.5167 - val_accuracy: 0.5167 - val_loss: 1.9215\n",
      "Epoch 9/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - Precision: 0.8579 - Recall: 0.8282 - accuracy: 0.8442 - loss: 0.4371 - val_Precision: 0.7895 - val_Recall: 0.7500 - val_accuracy: 0.7667 - val_loss: 0.6400\n",
      "Epoch 10/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 2s/step - Precision: 0.8515 - Recall: 0.8164 - accuracy: 0.8352 - loss: 0.4188 - val_Precision: 0.7667 - val_Recall: 0.7667 - val_accuracy: 0.7667 - val_loss: 0.5300\n",
      "Found 150 images belonging to 3 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2s/step - Precision: 0.7173 - Recall: 0.7040 - accuracy: 0.7062 - loss: 0.8402\n",
      "Test Accuracy: 0.7266666889190674\n",
      "Test Precision: 0.7448275685310364\n",
      "Test Recall: 0.7200000286102295\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on test data (you can adjust this depending on your test dataset)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(test_path,\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='categorical',\n",
    "                                                 color_mode='grayscale')\n",
    "\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "print(f\"Test Precision: {test_precision}\")\n",
    "print(f\"Test Recall: {test_recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MOBILENETV2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Found class labels: ['Healthy', 'Powdery', 'Rust']\n",
      "Encoded labels: [0 1 2]\n",
      "Number of training samples: 1322\n",
      "Number of validation samples: 60\n",
      "Number of test samples: 150\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import applications, models, layers, optimizers\n",
    "\n",
    "def preprocess_images(data_dir, target_size=(224, 224), color_mode='rgb'):\n",
    "    \"\"\"\n",
    "    Preprocesses the images in the given directory.\n",
    "    - Resizes images to `target_size`.\n",
    "    - Converts to grayscale if `color_mode` is 'grayscale'.\n",
    "    - Normalizes pixel values to [0, 1].\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    class_labels = os.listdir(data_dir)  # Folder names are the labels\n",
    "    print(f\"Found class labels: {class_labels}\")\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(class_labels)\n",
    "    print(f\"Encoded labels: {encoded_labels}\")\n",
    "\n",
    "    for label, encoded_label in zip(class_labels, encoded_labels):\n",
    "        folder_path = os.path.join(data_dir, label)\n",
    "        if not os.path.exists(folder_path) or not os.listdir(folder_path):\n",
    "            print(f\"Warning: Folder {folder_path} is empty or does not exist.\")\n",
    "            continue\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                if os.path.isfile(file_path):\n",
    "                    img = Image.open(file_path)\n",
    "                    if color_mode == 'grayscale':\n",
    "                        img = img.convert('L')  # Convert to grayscale\n",
    "                    elif color_mode == 'rgb':\n",
    "                        img = img.convert('RGB')\n",
    "                    img = img.resize(target_size)\n",
    "                    X.append(np.array(img))\n",
    "                    y.append(encoded_label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    if not X or not y:\n",
    "        raise ValueError(f\"No valid data found in directory: {data_dir}\")\n",
    "    \n",
    "    X = np.array(X).astype('float32') / 255.0  # Normalize pixel values\n",
    "    y = to_categorical(y)  # One-hot encode labels\n",
    "    return X, y, label_encoder\n",
    "\n",
    "train_path = 'C:\\Programming\\Machine learning\\plant images\\Train\\Train'\n",
    "valid_path = 'C:\\Programming\\Machine learning\\plant images\\Validation\\Validation'\n",
    "test_path = 'C:\\Programming\\Machine learning\\plant images\\Test\\Test'\n",
    "\n",
    "# Preprocess datasets\n",
    "X_train, y_train, label_encoder = preprocess_images(train_path, target_size=(224, 224), color_mode='rgb')\n",
    "X_valid, y_valid, _ = preprocess_images(valid_path, target_size=(224, 224), color_mode='rgb')\n",
    "X_test, y_test, _ = preprocess_images(test_path, target_size=(224, 224), color_mode='rgb')\n",
    "\n",
    "print(f\"Number of training samples: {X_train.shape[0]}\")\n",
    "print(f\"Number of validation samples: {X_valid.shape[0]}\")\n",
    "print(f\"Number of test samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 502ms/step - accuracy: 0.4092 - loss: 1.2972 - precision: 0.4284 - recall: 0.3281 - val_accuracy: 0.8667 - val_loss: 0.5195 - val_precision: 0.9375 - val_recall: 0.7500\n",
      "Epoch 2/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 560ms/step - accuracy: 0.7131 - loss: 0.6691 - precision: 0.7515 - recall: 0.6402 - val_accuracy: 0.9500 - val_loss: 0.3161 - val_precision: 0.9643 - val_recall: 0.9000\n",
      "Epoch 3/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 548ms/step - accuracy: 0.8002 - loss: 0.4715 - precision: 0.8410 - recall: 0.7704 - val_accuracy: 0.9500 - val_loss: 0.2260 - val_precision: 0.9500 - val_recall: 0.9500\n",
      "Epoch 4/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 509ms/step - accuracy: 0.8622 - loss: 0.3847 - precision: 0.8868 - recall: 0.8314 - val_accuracy: 0.9500 - val_loss: 0.1837 - val_precision: 0.9500 - val_recall: 0.9500\n",
      "Epoch 5/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 549ms/step - accuracy: 0.8885 - loss: 0.3193 - precision: 0.9095 - recall: 0.8610 - val_accuracy: 0.9500 - val_loss: 0.1532 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 6/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 556ms/step - accuracy: 0.8961 - loss: 0.3095 - precision: 0.9102 - recall: 0.8729 - val_accuracy: 0.9667 - val_loss: 0.1299 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 7/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 516ms/step - accuracy: 0.9126 - loss: 0.2578 - precision: 0.9203 - recall: 0.8983 - val_accuracy: 0.9667 - val_loss: 0.1339 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 8/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 546ms/step - accuracy: 0.9137 - loss: 0.2336 - precision: 0.9238 - recall: 0.9053 - val_accuracy: 0.9500 - val_loss: 0.1151 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 9/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 613ms/step - accuracy: 0.9172 - loss: 0.2311 - precision: 0.9241 - recall: 0.9093 - val_accuracy: 0.9500 - val_loss: 0.1113 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 10/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 486ms/step - accuracy: 0.9289 - loss: 0.2078 - precision: 0.9406 - recall: 0.9175 - val_accuracy: 0.9667 - val_loss: 0.0989 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 11/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 468ms/step - accuracy: 0.9331 - loss: 0.2066 - precision: 0.9370 - recall: 0.9217 - val_accuracy: 0.9667 - val_loss: 0.0970 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 12/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 527ms/step - accuracy: 0.9417 - loss: 0.1884 - precision: 0.9516 - recall: 0.9311 - val_accuracy: 0.9667 - val_loss: 0.0914 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 13/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 506ms/step - accuracy: 0.9251 - loss: 0.2265 - precision: 0.9291 - recall: 0.9190 - val_accuracy: 0.9667 - val_loss: 0.0964 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 14/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 499ms/step - accuracy: 0.9655 - loss: 0.1399 - precision: 0.9691 - recall: 0.9585 - val_accuracy: 0.9667 - val_loss: 0.0909 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 15/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 483ms/step - accuracy: 0.9378 - loss: 0.1992 - precision: 0.9443 - recall: 0.9333 - val_accuracy: 0.9667 - val_loss: 0.0852 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 16/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 567ms/step - accuracy: 0.9453 - loss: 0.1608 - precision: 0.9493 - recall: 0.9412 - val_accuracy: 0.9667 - val_loss: 0.0824 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 17/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 666ms/step - accuracy: 0.9513 - loss: 0.1421 - precision: 0.9557 - recall: 0.9473 - val_accuracy: 0.9667 - val_loss: 0.0877 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "Epoch 18/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 633ms/step - accuracy: 0.9507 - loss: 0.1563 - precision: 0.9582 - recall: 0.9467 - val_accuracy: 0.9667 - val_loss: 0.0863 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 19/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 564ms/step - accuracy: 0.9582 - loss: 0.1436 - precision: 0.9662 - recall: 0.9544 - val_accuracy: 0.9667 - val_loss: 0.0805 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 20/20\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 564ms/step - accuracy: 0.9642 - loss: 0.1088 - precision: 0.9691 - recall: 0.9580 - val_accuracy: 0.9667 - val_loss: 0.0808 - val_precision: 0.9661 - val_recall: 0.9500\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 332ms/step - accuracy: 0.9733 - loss: 0.1409 - precision: 0.9754 - recall: 0.9733\n",
      "Test accuracy: 0.9667\n",
      "Test precision: 0.9732\n",
      "Test recall: 0.9667\n",
      "Epoch 1/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 2s/step - accuracy: 0.7576 - loss: 0.6253 - precision: 0.7732 - recall: 0.7285 - val_accuracy: 0.9667 - val_loss: 0.1013 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 2/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 2s/step - accuracy: 0.8938 - loss: 0.3104 - precision: 0.9089 - recall: 0.8763 - val_accuracy: 0.9667 - val_loss: 0.1319 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 3/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.9474 - loss: 0.1779 - precision: 0.9548 - recall: 0.9384 - val_accuracy: 0.9667 - val_loss: 0.1482 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 4/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m89s\u001b[0m 2s/step - accuracy: 0.9417 - loss: 0.1731 - precision: 0.9467 - recall: 0.9327 - val_accuracy: 0.9667 - val_loss: 0.1539 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 5/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 2s/step - accuracy: 0.9580 - loss: 0.1312 - precision: 0.9634 - recall: 0.9476 - val_accuracy: 0.9667 - val_loss: 0.1589 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 6/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 2s/step - accuracy: 0.9570 - loss: 0.1219 - precision: 0.9637 - recall: 0.9540 - val_accuracy: 0.9667 - val_loss: 0.1586 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 7/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 2s/step - accuracy: 0.9701 - loss: 0.1267 - precision: 0.9741 - recall: 0.9643 - val_accuracy: 0.9667 - val_loss: 0.1592 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 8/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 2s/step - accuracy: 0.9709 - loss: 0.0983 - precision: 0.9717 - recall: 0.9672 - val_accuracy: 0.9667 - val_loss: 0.1582 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 9/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 2s/step - accuracy: 0.9778 - loss: 0.0737 - precision: 0.9778 - recall: 0.9769 - val_accuracy: 0.9667 - val_loss: 0.1491 - val_precision: 0.9667 - val_recall: 0.9667\n",
      "Epoch 10/10\n",
      "\u001b[1m42/42\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 2s/step - accuracy: 0.9836 - loss: 0.0553 - precision: 0.9846 - recall: 0.9811 - val_accuracy: 0.9667 - val_loss: 0.1401 - val_precision: 0.9667 - val_recall: 0.9667\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Load a pretrained model (transfer learning)\n",
    "base_model = applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(3, activation='softmax')  # 3 classes: Healthy, Powdery, Rust\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    epochs=20,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test precision: {test_precision:.4f}\")\n",
    "print(f\"Test recall: {test_recall:.4f}\")\n",
    "\n",
    "# Fine-tuning: Unfreeze some layers of the base model and retrain\n",
    "base_model.trainable = True\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.00001),  # Lower learning rate\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "fine_tuning_history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=32),\n",
    "    epochs=10,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 309ms/step - accuracy: 0.9565 - loss: 0.2573 - precision: 0.9565 - recall: 0.9565\n",
      "Final accuracy after fine-tuning: 0.9333\n",
      "Final precision: 0.9333\n",
      "Final recall: 0.9333\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_accuracy, final_precision, final_recall = model.evaluate(X_test, y_test)\n",
    "print(f\"Final accuracy after fine-tuning: {final_accuracy:.4f}\")\n",
    "print(f\"Final precision: {final_precision:.4f}\")\n",
    "print(f\"Final recall: {final_recall:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
